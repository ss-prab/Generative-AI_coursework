# -*- coding: utf-8 -*-
"""FINAL_Sneha_ANN_Week_3_Assignment (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16AhEVlXeBtRrLW3iAnkrRagwBA0jou0s

**Candidate: Sneha Santha Prabakar**


**Assignment: Week 3 - ANN**
"""

from google.colab import drive

drive.mount('/content/drive')

import os
folder_path = '/content/drive/My Drive/NUS-GenAI'
os.chdir(folder_path)

import warnings
warnings.filterwarnings('ignore')

"""# Section 1: Import data and libraries

## Import libraries
"""

# Data manipulation
import pandas as pd
import numpy as np

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Sklearn - preprocessing, model selection, and evaluation
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix,
    ConfusionMatrixDisplay, roc_curve, auc
)

# Handling class imbalance
from imblearn.over_sampling import SMOTE

# TensorFlow/Keras - model building
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Conv1D, GlobalMaxPooling1D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import regularizers
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau

import random
import tensorflow as tf
tf.random.set_seed(42)
random.seed(42)
np.random.seed(42)

"""## Import dataset"""

# import dataset

df = pd.read_csv('enhanced_diabetes_dataset.csv')
df.head()

"""## Overview of the dataset

**Size of the dataset**
"""

df.shape

"""The dataset has 17 columns and 2,000 rows. This is a reasonable data size to build a neural network.

**Data type of the columns**
"""

columns_datatype = df.dtypes.reset_index()
columns_datatype.columns = ['Column', 'Data type']
columns_datatype

# num unique values per column
df.nunique()

"""We have a dataset containing **16 features** and 1 independent variable (*Diabetes*).

All the features are numeric (with a mix of integer and float data types).

The independent variable, also the target variable of this analysis, is a binary column. Hence, this is a **classification** problem.

The features can be broadly divided into the following categories:

1. General characteristics of the individual
  * Age
  * Gender
  * BMI
  * Skin thickness
  * Smoking
  * Physical activity
2. Diabetes markers
  * Glucose
  * Insulin
  * Diabetes Pedigree Function
  * HbA1c
  * FastingBS
  * Prediabetes
3. Other health conditions
  * Blood Pressure
  * Pregnancies
  * Triglycerides
  * HDL

**Summary of Statistics of the dataset**
"""

# Basic information of the dataset

# drop the categorical columns (Gender, Smoking, Prediabetes, Diabetes)
df_numeric = df.drop(columns=['Gender', 'Smoking', 'Prediabetes', 'Diabetes'])

# Display the statistics for the numeric columns
df_numeric.describe()

"""Statistical summary makes sense only for continuous variables. Hence, we remove the independent variable and the categorical variables while performing the statistical summary analysis. This leaves us with **13 continuous variable** columns.

We can make the following observations from the statistical summary of the continuous variable columns:

*   The count of all features is not 2000, some features have count lesser than 2000. This indicates the **presence of missing values** in certain features.
*   The following features have a mean that is approximately equal to the median (indicating a uniform normal distribution):
  1. Age
  2. Glucose
  3. Blood Pressure
  4. Skin thickness
  5. BMI
  6. HbA1c
  7. FastingBS
  8. Triglycerides
  9. HDL
* The following features indicate a skewed distribution:
  1. Insulin | Mean > Median | **Right-skewed** distribution
  2. DiabetesPedigreeFunction | Mean > Median | **Right-skewed** distribution
  3. PhysicalActivity | Mean slightly more than the Median | *Lightly right-skewed* distribution
  4. Pregnancies | Mean slightly more than the Median | *Lightly right-skewed* distribution
* Features like Insulin and Triglycerides have very high standard deviation compared to their mean which may suggest a **high spread**/variance.
* Contextual analysis: The minimum value of Insulin is 0. This may indicate an **error in data reporting** or a missing value encoding, as it is medically not possible for anyone to have Insuling as 0 mu U/ml.
* Presence of **outliers** - some features such as Insulin, have very high max value compared to their median, which may indicate the presence of outliers.

# Section 2: EDA Visualization

## Distribution plots comparing features between Diabetic and Non-Diabetic patients
"""

diabetic_patients = df[df['Diabetes'] == 1]
non_diabetic_patients = df[df['Diabetes'] == 0]

print(f'Number of diabetic patients: {len(diabetic_patients)}')
print(f'Number of non-diabetic patients: {len(non_diabetic_patients)}')

perc_diabetic = (len(diabetic_patients) / len(df)) * 100
perc_non_diabetic = (len(non_diabetic_patients) / len(df)) * 100

print(f'Percentage of diabetic patients: {perc_diabetic:.1f}%')
print(f'Percentage of non-diabetic patients: {perc_non_diabetic:.1f}%')

ax = sns.countplot(x='Diabetes', data=df)
plt.title('Distribution of Diabetic and Non-Diabetic Patients')
plt.xlabel('Diabetes')
plt.ylabel('Count')

# adding count label on top of the bars
for p in ax.patches:
    ax.annotate(f'{int(p.get_height())}',
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center',
                xytext=(0, 5),
                textcoords='offset points')

plt.show()

"""We can see that the dataset is **heavily imbalanced** - the number of patients **with diabetes** make up a staggering **majority** (80.4%) while the number of patients **without diabetes** are **very few** in comparison (only 19.6%). Using such data to train a machine learning model will make the model biased towards patients with diabetes and it will perform poorly to correctly detect patients without diabetes. Hence, this imablance of classes in the dataset needs to be handled before we start training the model.

Let us also compare the distribution of features across both the classes.

For integer features, we will plot the bar chart to see the distribution.

For continuous (numeric) features, we will plot the density plots to see the distribution.
"""

df.dtypes

int_features = df.dtypes[df.dtypes == 'int64'].index.to_list()
int_features.remove('Diabetes') # since Diabetes is not a feature; it is the independent variable

cont_features = df.dtypes[df.dtypes == 'float64'].index.to_list()

print(f'Integer features: {int_features}')
print(f'Continuous features: {cont_features}')

# Part 1: Integer features distribution - Bar plot

## !! Add a curve on top of the bars, if possible

for feature in int_features:

  fig = px.histogram(df,
                   x=feature,
                   color='Diabetes',
                   barmode='group',
                   text_auto=True,
                   color_discrete_map={0: 'blue', 1: 'orange'})

  fig.update_layout(title=f'Distribution of {feature} by Diabetes',
                    xaxis_title=feature,
                    yaxis_title='Count')

  fig.show()

# Distribution for continous features

for feature in cont_features:
    fig = px.histogram(df,
                   x=feature,
                   color='Diabetes',
                   facet_col='Diabetes',
                   histnorm='probability density',
                   opacity=0.7)

    fig.update_layout(
        title=f'Distribution of {feature} by Diabetes Status',
        xaxis_title=feature,
        yaxis_title='Density',
        plot_bgcolor='white',
        paper_bgcolor='white',
        showlegend=False
    )

    fig.show()

"""## Correlation Analysis"""

df_corr = df.drop(columns=['Diabetes'])
corr_matrix = df_corr.corr()

plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix,
            annot=True,         # show correlation values
            fmt='.2f',           # format to 2 decimal places
            cmap='coolwarm',     # color map
            square=True,         # make cells square
            linewidths=0.5)      # line width between squares
plt.title('Correlation Heatmap of Features')
plt.show()

"""We can see that the following features have high positive correlation (>=0.7):



*   Gender and Pregnancies (0.7)
*   HbA1c and Glucose (0.73)
*   FastingBS and Glucose (0.91)

FastingBS and Glucose also have a strong positive correlation (0.67), but it is still less than 0.7 and hence, not strong enough to cause multi-collinearity.

We need to remove strongly correlated features in order to avoid multi-collinearity in our model. This means that in each pair of the strongly correlated features, we need to remove one feature.

We can do a Feature Importance analysis in order to choose which features to keep and which to remove.

## Feature Importance

We use a Random Forest model in order to determine the feature importance because:



*   It **captures non-linear relationships** between the features and the target well.
*   It can handle **both int and continous variables**.
*   It is **robust to outliers** (this is important since we have some features, such as HbA1c which have outliers)
*   It is fast to train, works well on large datasets, and **easy to interpret feature importance** from the model
*   Reliable result as:
  - It builts multiple trees
  - Every time a tree is built, it chooses splits that reduce the impurity and better features are used more often and early on, in the trees.
  - It then sums how much each feature reduced impurity across all trees
  - Then averages this to give the final feature importance score.
"""

X = df.drop(columns='Diabetes')
y = df['Diabetes']

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X, y)

importances = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False)

plt.figure(figsize=(12, 8))
importances.plot(kind='bar')
plt.title('Feature Importance from Random Forest')
plt.ylabel('Importance Score')
plt.show()

"""We can clearly see that the top 5 most important features are:

1. HbA1c
2. Glucose
3. Triglycerides
4. FastingBS
5. BMI


The least important features are:
1. Smoking
2. Gender
3. Prediabetes


Earlier we found out that the following pairs have high-correlation:
1. Gender and Pregnancies (0.7)
2. HbA1c and Glucose (0.73)
3. FastingBS and Glucose (0.91)


Using our knowledge from the correlation analysis and feature importance, we can determine that the following features can be removed (to avoid multi-collinearity in the model):

1. Gender - it is anyway a low-importance feature
2. Glucose - though it is rated high in feature importance, HbA1c has a much higher feature importance score than all the features. So between HbA1c and Glucose, we choose to remove Glucose. Removing Glucose would also break the multi-collinearity between FastingBS and Glucose.

Let us re-plot the correlation matrix after removing these two features, to make sure there is no multi-collinearity in the dataset anymore.
"""

removed_cols = df[['Gender', 'Glucose']]
df.drop(columns=['Gender', 'Glucose'], inplace=True)

df_corr = df.drop(columns=['Diabetes'])
corr_matrix = df_corr.corr()

plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix,
            annot=True,         # show correlation values
            fmt='.2f',           # format to 2 decimal places
            cmap='coolwarm',     # color map
            square=True,         # make cells square
            linewidths=0.5)      # line width between squares
plt.title('Correlation Heatmap of Features')
plt.show()

"""We can see that there is no more multi-collinearity in the dataset anymore.

# Section 3: Data Quality

## Check for empty records
"""

missing_values = df.isna().sum()
missing_values = missing_values[missing_values > 0].sort_values(ascending=False)

missing_summary = pd.concat([missing_values,
                             (missing_values / len(df) * 100).map(lambda x: f'{x:.1f}%')
                             ],
                            axis=1)
missing_summary.columns = ['Count of missing values', 'Percentage of dataset']

missing_summary

"""We can see that the missing values make up roughly 5% of the dataset. Though it is not too significant, it is not very small either. Hence, we will retain these records and impute the missing values with either mean or median of the feature.


Let us review the statistical summary of these features to determine the best method for imputing the missing values:
"""

df_missing_cols = df[['PhysicalActivity', 'BloodPressure', 'HDL', 'Triglycerides', 'SkinThickness']]
df_missing_cols.describe()

"""Based on the statistical summary, we can impute the missing values with mean for the features with uniform distribution (where mean is approx equal to median) - BloodPressure, SkinThickness.

We can impute the missing values with median for the features with slightly skewed distribution (where mean is lesser or greater than the median) - PhysicalActivity, HDL, Triglycerides.
"""

# Mean imputation

mean_imputation_cols = ['BloodPressure', 'SkinThickness']
df[mean_imputation_cols] = df[mean_imputation_cols].fillna(df[mean_imputation_cols].mean())

# Median imputation

median_imputation_cols = ['PhysicalActivity', 'HDL', 'Triglycerides']
df[median_imputation_cols] = df[median_imputation_cols].fillna(df[median_imputation_cols].median())

df.isna().sum()

"""There are no missing values in the dataset anymore.

Let us also check for zero-values in the dataset and verify that they are medically and statistically logical.
"""

zero_stats = (df == 0).sum()
zero_stats = zero_stats[zero_stats > 0].sort_values(ascending=False)
print(zero_stats)

df.describe()

"""It is natural to have 0 value in most of these columns, except Insulin:

- Smoking, Prediabetes, Diabetes: Binary variables where 0 indicates the absence of the condition in the person
- Pregnancies: It is perfectly logical for a person to have 0 pregnancies in their medical history
- Physical Activity: Value = 0 refers to people who have no physical activity; also realistic.


It is unnatural for people to have insulin level = 0, even if they are diabetic. Some people with Type 1 Diabetes may have very low insulin close to 0, but it is almost not possible for anyone to have insulin=0. Hence, let us dig deeper into records where insulin = 0 to see if it is a genuine data or a case of incorrectly reported data.
"""

zero_insulin_df = df[df['Insulin'] == 0]
print(f'Num records with Insulin = 0: {zero_insulin_df.shape[0]}')
print(f'Percentage of records with Insulin = 0: {len(zero_insulin_df) / len(df) * 100:.1f}%')

zero_insulin_diabetic = zero_insulin_df[zero_insulin_df['Diabetes'] == 1]
print(f'Num Diabetic patients with Insulin = 0: {zero_insulin_diabetic.shape[0]}')

zero_insulin_non_diabetic = zero_insulin_df[zero_insulin_df['Diabetes'] == 0]
print(f'Num non-Diabetic patients with Insulin = 0: {zero_insulin_non_diabetic.shape[0]}')

"""A significant proportion of records (36.9%) have patients with Insulin = 0. Hence, we cannot ignore this case.

It is surprising that 158 patients (21% of these records) are non-diabetic patients - which is clinically not possible. It is possible that 0 will simply used to impute the records with missing values for this column. However, it is not a logical imputation as it does not make sense medically and it will mislead the model.

Hence, we first revert these 0 values to NA, and find a logically correct way to impute these records.
"""

df['Insulin'] = df['Insulin'].replace(0, np.nan)
median_insulin = df['Insulin'].median()
df['Insulin'] = df['Insulin'].fillna(median_insulin)

"""Let us also check other features where there might be 0 values in the records, but medically it does not make sense to have it.

Features, other than Insulin, where it does not make sense to have 0 readings are: 'BloodPressure', 'SkinThickness', 'BMI','HbA1c', 'FastingBS', 'Triglycerides', 'HDL'
"""

non_zero_cols = ['BloodPressure', 'SkinThickness', 'BMI','HbA1c', 'FastingBS', 'Triglycerides', 'HDL']

for col in non_zero_cols:
  num_zero = df[df[col]==0].shape[0]
  print(f'Num records with {col} = 0: {num_zero}')

df.isna().sum()

"""We can see that none of these fields have suspicious zeros. We have also confirmed that there are no more missing records.

## Check for Outliers

So far - we have removed two columns - Gender and Glucose - from the dataset, to avoid multi-collinearity.

And we have handled missing values and zeroes in the features.

We now have 14 features for our model. The next step in checking data quality is handling Outliers.
"""

len(df.drop(columns=['Diabetes']).columns)

"""We will use **Boxplot** to detect outliers"""

features_df = df.drop(columns=['Diabetes'])
plt.figure(figsize=(30, 30))
sns.boxplot(data=features_df, orient='h')
plt.title('Boxplots of all descriptive variables', fontsize=50)
plt.xticks(fontsize=30)
plt.yticks(fontsize=30)

plt.tight_layout()
plt.show()

"""The boxplot reveals that almost all features except Age and Prediabetes have outliers (data points outside the whisker).


We have already taken care of cases where the feature having value = 0 is biologically not possible.

We can also Cap extreme high-end outliers (winsorization) at the 99th percentile. This means we are limiting extreme values to a threshold (the 99th percentile), without
not deleting or excluding those records. This is important because, for example, outliers like Insulin > 500 distort ANN training. Following the winsorization approach, we still retain these records (as they may be medically relevant) while also limiting any extreme effects on our model training.

Performing standardization after this will further stabilize the dataset for model training.
"""

feature_cols = df.drop(columns=['Diabetes']).columns

for col in feature_cols:
    cap = df[col].quantile(0.99)
    df[col] = np.where(df[col] > cap, cap, df[col])

df.drop(columns=['Diabetes']).describe()

"""# Section 4: Data Pre-Processing

The Data Pre-Processing steps are:



1.   Split the dataset into features (X) and target variable (Y)
2.   Split the features and target variable into 70% train, 15% val, 15% test
  - It is necessary to split the dataset into trian-test-validation sets before we do Standardization, because we are supposed to "fit" the Scaler only on Train data, not on Test/Validation data.
    - Scaling should only be fit on the training data to prevent data leakage.
If we scale the full dataset first, the scaler “sees” distribution of test/validation data, which leads to inflated model performance. *The Validation and test dataset should reflect real-word class distribution.*
  - We will perform Stratified splitting in order to ensure we have both the classes (Diabetic and non-diabetic patients) in all 3 datasets (train, test and validation)
3. Handle Class Imbalance - We already know that the dataset is heavily imbalanced. Hence, we will performe SMOTE sampling (only in the training set) to balance the classes while training the model (to avoid bias).
  - SMOTE should not be applied to test or validation sets because it synthetically creates samples and can skew evaluation metrics.
It should be used only to improve class balance during training.

## Split data
"""

# Drop columns that are not features
X = df.drop(columns=['Diabetes'])
y = df['Diabetes']

# Train-Test-Validation split (70% train, 15% val, 15% test)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, stratify=y, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=42)

"""## Handle Class Imbalance"""

# Apply SMOTE on the training set only
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

"""## Standardize the features"""

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_resampled)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

"""We can now check that:
1. All 3 sets - train/test/validation - have both diabetic and non-diabetic patients
2. The training data is well-balanced between both the classes.
3. The data split: 75-15-15 for train-validate-test sets is correct
"""

y_train_resampled.value_counts() # after SMOTE

y_val.value_counts()

y_test.value_counts()

"""# Section 5: Designing the model

Theoretically the model would perform better if designed as a deep neural network instead of CNN:

CNNs (Convolutional Neural Networks) are designed for:
- Image data or Spatial or grid-like structures (e.g., 2D images, 1D time series, video frames)
- Use filters/kernels to extract spatial features (edges, shapes, textures)

This dataset is tabular, with:
- Structured columns: 'Pregnancies', 'Glucose', 'BloodPressure', etc.
- Each row represents a single patient record — not a sequence, image, or spatial structure
- No local relationships or “spatial patterns” between features (e.g., Glucose and BMI aren’t adjacent pixels)


CNNs expect input with spatial dimensions (e.g., 28×28 pixel images with 3 channels), but this dataset is a flat vector of health metrics.

Applying convolution on such unrelated features confuses the model (e.g., sliding a 3×3 kernel across unrelated columns like Age, BMI, Insulin doesn't make sense).

ANNs work well for tabular/structured data, where all features contribute to prediction but don’t have spatial locality. And each feature in this dataset is independent, and relationships between them are best captured using dense connections.

We can do both CNN and ANN to compare the results:

## Model 1: CNN
"""

# Reshape for Conv1D input: (samples, features, 1)
X_train_cnn = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)
X_val_cnn = X_val_scaled.reshape(X_val_scaled.shape[0], X_val_scaled.shape[1], 1)
X_test_cnn = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)

cnn_model = Sequential([
    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),
    Dropout(0.2),
    GlobalMaxPooling1D(),
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])

cnn_model.compile(optimizer=Adam(learning_rate=0.001),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])

callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
    ModelCheckpoint('cnn_best_model.h5', monitor='val_loss', save_best_only=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)
]

cnn_history = cnn_model.fit(
    X_train_cnn, y_train_resampled,
    validation_data=(X_val_cnn, y_val),
    epochs=50,
    batch_size=32,
    callbacks=callbacks,
    verbose=1
)

plt.plot(cnn_history.history['loss'], label='Train Loss')
plt.plot(cnn_history.history['val_loss'], label='Val Loss')
plt.title("CNN Loss Over Epochs")
plt.legend()
plt.show()

plt.plot(cnn_history.history['accuracy'], label='Train Accuracy')
plt.plot(cnn_history.history['val_accuracy'], label='Val Accuracy')
plt.title("CNN Accuracy Over Epochs")
plt.legend()
plt.show()

"""Loss plot:
- Train Loss steadily decreases over epochs, which is expected and indicates the model is learning from the training data.

- Validation Loss, however, is consistently higher than train loss and fluctuates significantly, showing no clear downward trend.

- This gap and fluctuation suggest that the model is not generalizing well to unseen validation data, which is a clear sign of overfitting.

Accuracy plot:
- Train Accuracy increases gradually and reaches above 70%.

- Validation Accuracy fluctuates and stays much lower, stuck around 62%, with very little improvement over time.

- This widening gap between training and validation accuracy confirms that the model is memorizing the training data and failing to learn generalizable patterns.


Overfitting is evident.

Training performance improves, but validation performance stagnates, making the model unreliable for unseen data.

This could be due to:

- Model complexity (too many filters/layers).

- Inadequate regularization.

- Small or unbalanced dataset.

- Insufficient early stopping or dropout.






"""

# Use preprocessed and scaled versions for CNN input
X_train_cnn = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)
X_val_cnn = X_val_scaled.reshape(X_val_scaled.shape[0], X_val_scaled.shape[1], 1)

# Hyperparameters to test
learning_rates = [0.01, 0.001]
batch_sizes = [16, 32]
dropout_rates = [0.2]
filter_numbers = [32, 64]
kernel_sizes = [3]

results = []

# Grid search
for lr, bs, dr, fn, ks in itertools.product(learning_rates, batch_sizes, dropout_rates, filter_numbers, kernel_sizes):
    model = Sequential([
        Conv1D(filters=fn, kernel_size=ks, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),
        Dropout(dr),
        GlobalMaxPooling1D(),
        Dense(32, activation='relu'),
        Dropout(dr),
        Dense(1, activation='sigmoid')
    ])

    model.compile(optimizer=Adam(learning_rate=lr), loss='binary_crossentropy', metrics=['accuracy'])

    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

    history = model.fit(X_train_cnn, y_train_resampled, validation_data=(X_val_cnn, y_val),
                        epochs=20, batch_size=bs, callbacks=[early_stop], verbose=0)

    val_preds = model.predict(X_val_cnn)
    val_preds_binary = (val_preds > 0.5).astype(int)
    val_acc = accuracy_score(y_val, val_preds_binary)

    results.append({
        'Learning Rate': lr,
        'Batch Size': bs,
        'Dropout Rate': dr,
        'Filter Number': fn,
        'Kernel Size': ks,
        'Final Val Accuracy': val_acc
    })

results_df = pd.DataFrame(results).sort_values(by='Final Val Accuracy', ascending=False)
results_df

"""We can see that the best CNN configuration is achieved with:

- learning rate = 0.001
- Batch size = 32
- Dropout = 0.2
- Filter number = 64
- Kernel Size = 3
"""

# Reshape data for Conv1D
X_train_cnn = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)
X_val_cnn = X_val_scaled.reshape(X_val_scaled.shape[0], X_val_scaled.shape[1], 1)
X_test_cnn = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)

# Final CNN Model
cnn_final = Sequential([
    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),
    Dropout(0.2),
    GlobalMaxPooling1D(),
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])

# Compile
cnn_final.compile(optimizer=Adam(learning_rate=0.001),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])

# Callbacks
callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
    ModelCheckpoint('cnn_best_model_final.h5', monitor='val_loss', save_best_only=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)
]

# Train
cnn_history = cnn_final.fit(
    X_train_cnn, y_train_resampled,
    validation_data=(X_val_cnn, y_val),
    epochs=30,
    batch_size=32,
    callbacks=callbacks,
    verbose=1
)

# Loss
plt.plot(cnn_history.history['loss'], label='Train Loss')
plt.plot(cnn_history.history['val_loss'], label='Val Loss')
plt.title("Final CNN - Loss Over Epochs")
plt.legend()
plt.show()

# Accuracy
plt.plot(cnn_history.history['accuracy'], label='Train Accuracy')
plt.plot(cnn_history.history['val_accuracy'], label='Val Accuracy')
plt.title("Final CNN - Accuracy Over Epochs")
plt.legend()
plt.show()

"""1. Loss Curve:
- The training loss decreases consistently.

- The validation loss fluctuates, suggesting some instability in generalization.

- A growing gap or fluctuation indicates potential overfitting or noise sensitivity.

2. Accuracy Curve:
- Training accuracy improves steadily, reaching around 70%.

- Validation accuracy fluctuates and doesn’t consistently improve — stuck around 63–64%.

- This shows unstable learning and likely indicates the model is not generalizing well.
"""

# Reshape test data
X_test_cnn = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)

# Evaluate
test_loss, test_acc = cnn_final.evaluate(X_test_cnn, y_test)
print(f"Test Accuracy: {test_acc:.4f}")

# Predictions & Confusion Matrix

y_pred_probs = cnn_final.predict(X_test_cnn)
y_pred = (y_pred_probs > 0.5).astype(int)

print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.title("CNN Confusion Matrix - Test Set")
plt.show()


# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 5))
plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.grid(True)
plt.show()

"""- AUC of 0.74 means the model has 74% chance of distinguishing between a diabetic and non-diabetic patient. It's better than random guessing, but not strong (as it is still below 0.8).

- Class 1 (Diabetic):

  - Recall = 0.75 → 75% of diabetic patients were correctly detected.

  - Precision = 0.81 → When model predicts diabetes, it's correct 81% of the time.
  
  This is fairly strong performance, and it's especially important in a medical context to catch as many diabetic cases as possible.

- Class 0 (Non-Diabetic):

  - Recall = 0.31 → Only 31% of non-diabetics were correctly identified.

  - Precision = 0.23 → Very low. Majority of non-diabetic predictions are wrong.

  This means the model struggles with detecting non-diabetics and has a high false positive rate for them.



- Class imbalance may still be affecting the model.

- CNN appears biased toward class 1, possibly due to how it interprets patterns in tabular input. If misclassifying non-diabetics has high cost (e.g. unnecessary stress, tests), this model needs improvement.

## Model 2: ANN

To start with, we will use the following parameters:

- Number of hidden layers: 1
  - We start with only 1 hidden layer because our data size is relatively small (2.2K records). With more hidden layers for a relatively small dataset, the chances of overfitting increases.

- Input layer:
  - Number of neurons: 64
  - Activation function: ReLU
  - Regularisation: L2 (0.001)
  - Dropout: 0.2
- Hidden layer:
  - Number of neurons: 32
  - Activation function: ReLU
  - Regularisation: L2 (0.001)
  - Dropout: 0.2
- Output layer:
  - Number of neurons: 1 (since we have only 1 output field)
  - Activation function: Sigmoid (binary classification)

- Loss function:Binary cross-entropy
- Evaluation metric:Accuracy
- Optimiser: Adam
- Learning rate: 0.001
"""

# Model definition
model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],),
          kernel_regularizer=regularizers.l2(0.001)),
    Dropout(0.2),
    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])

# Compile the model
optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer,
              loss='binary_crossentropy',
              metrics=['accuracy'])

callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)
]

history = model.fit(
    X_train_scaled, y_train_resampled,
    validation_data=(X_val_scaled, y_val),
    epochs=50,
    batch_size=32,
    callbacks=callbacks
)

# Plot training & validation loss
plt.figure(figsize=(10, 4))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# Plot training & validation accuracy
plt.figure(figsize=(10, 4))
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('Accuracy Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

"""1. Loss curve:
- Training Loss: Training loss consistently decreases, which is good—it shows the model is learning patterns from the training data.

- Validation Loss: Decreases initially and stabilizes after a few epochs, with minor fluctuations.

- This suggests that the model is not severely overfitting. It is generalizing reasonably well on the validation data. Only slight overfitting is indicated near the end, as validation loss doesn't continue to drop like training loss does—but the gap is small and acceptable.

2. Accuracy curve:
- Training Accuracy: Shows a steady upward trend — indicating that the model is learning effectively.

- Validation Accuracy: Also improves initially and then plateaus around ~73-74%, with minor variance across epochs.

This again suggests reasonable generalization and stable training.

3. In comparison to the CNN model, at this stage:

The ANN model showcases better overall performance than CNN (as seen in previous results). It has less variance between training and validation compared to CNN.


"""

def build_and_train_model(dropout_rate, learning_rate, batch_size):
    model = Sequential([
        Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),
        Dropout(dropout_rate),
        Dense(32, activation='relu'),
        Dropout(dropout_rate),
        Dense(1, activation='sigmoid')
    ])

    model.compile(optimizer=Adam(learning_rate=learning_rate),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])

    # early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

    callbacks_ = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3) ]

    history = model.fit(X_train_scaled, y_train_resampled,
                        validation_data=(X_val_scaled, y_val),
                        epochs=50,
                        batch_size=batch_size,
                        callbacks=callbacks_,
                        verbose=0)

    val_acc = history.history['val_accuracy'][-1]
    return {'Dropout Rate': dropout_rate, 'Learning Rate': learning_rate,
            'Batch Size': batch_size, 'Final Val Accuracy': val_acc}

# Run tuning
results = []
for dr in [0.2, 0.4]:
    for lr in [0.001, 0.0005]:
        for bs in [16, 32]:
            print(f"Testing: Dropout={dr}, LR={lr}, Batch Size={bs}")
            results.append(build_and_train_model(dr, lr, bs))

# View results
tuning_df = pd.DataFrame(results)
tuning_df.sort_values(by='Final Val Accuracy', ascending=False)

"""The best hyperparameters are:
dropout = 0.4,
learning rate = 0.0005,
batch size = 16,
"""

# Best hyperparameters from tuning
best_dropout = 0.4
best_lr = 0.0005
best_batch_size = 16

# Re-train model using best parameters

# Build final model
final_model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    Dropout(best_dropout),
    Dense(32, activation='relu'),
    Dropout(best_dropout),
    Dense(1, activation='sigmoid')
])

final_model.compile(optimizer=Adam(learning_rate=best_lr),
                    loss='binary_crossentropy',
                    metrics=['accuracy'])

# Callbacks
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train final model
final_history = final_model.fit(
    X_train_scaled, y_train_resampled,
    validation_data=(X_val_scaled, y_val),
    epochs=50,
    batch_size=best_batch_size,
    callbacks=[early_stop],
    verbose=1
)

# Plot loss
plt.figure(figsize=(10,4))
plt.plot(final_history.history['loss'], label='Train Loss')
plt.plot(final_history.history['val_loss'], label='Val Loss')
plt.title("Final Model - Loss Over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()

# Plot accuracy
plt.figure(figsize=(10,4))
plt.plot(final_history.history['accuracy'], label='Train Accuracy')
plt.plot(final_history.history['val_accuracy'], label='Val Accuracy')
plt.title("Final Model - Accuracy Over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

"""These final ANN training plots actually show a well-regularized and stable model after hyperparameter tuning.

1. Loss Plot Analysis
  - Train Loss: Decreases consistently throughout training, with a gradual flattening — a good sign that the model is learning effectively.

  - Validation Loss: Also steadily decreases with minor fluctuations, and does not show any steep increase, which means overfitting is controlled.

  - Conclusion: The regularization (Dropout + L2), early stopping, and learning rate choices are working well.

2. Accuracy Plot Analysis
  - Train Accuracy: Rises steadily and crosses 0.75 — expected as model sees the same data.

  - Validation Accuracy: Peaks around 0.73, then stays stable with minor drops.

  - The gap between training and validation accuracy is small, which suggests low variance and a well-generalized model. This model is not overfitting and generalizes well to unseen data.




"""

# Predict on test set
y_pred_proba = model.predict(X_test_scaled).ravel()
y_pred = (y_pred_proba > 0.5).astype(int)

# Classification Report
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0,1], yticklabels=[0,1])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 5))
plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.grid(True)
plt.show()

"""- AUC = 0.72
This indicates moderate discriminative power. AUC of 0.5 is random guessing, and 1.0 is perfect classification. So, 0.72 suggests the model is doing better than random but has room to improve.

- The ROC curve rises above the diagonal, which is good.

- Class 1 (Diabetic)

  - Precision: 0.87 — very strong; when the model predicts diabetes, it is usually right.

  - Recall: 0.76 — good; it correctly identifies 76% of diabetic patients.

  - F1-Score: 0.81 — strong balance between precision and recall.

  - Conclusion: Class 1 is well-handled.

- Class 0 (Non-diabetic)

  - Precision: 0.35 — poor; many non-diabetics are wrongly predicted as diabetic.

  - Recall: 0.53 — mediocre; nearly half the non-diabetics are missed.

  - F1-Score: 0.42 — quite low.

  - Conclusion: The model struggles to detect non-diabetic patients.

## Conclusion

- AUC: The CNN has a slightly higher AUC (0.74 vs. 0.72), indicating it has marginally better overall ranking ability. However, this difference is small and does not reflect better practical classification.

- Class-wise performance:

  - Diabetic (Class 1): Both models perform reasonably well, but ANN outperforms CNN in both precision and recall.

  - Non-diabetic (Class 0): ANN significantly outperforms CNN, especially in recall (53% vs. 31%). CNN struggles to identify non-diabetics.

  - Overall Accuracy: ANN leads with 72% vs. CNN’s 66%, indicating better general performance.

Hence, we can conclude that while the CNN model has a slightly better AUC, the ANN model performs more effectively in real-world classification, especially for both diabetic and non-diabetic patients. ANN yields higher precision and recall for both classes and better overall accuracy, making it the more suitable model for this binary classification task.
"""
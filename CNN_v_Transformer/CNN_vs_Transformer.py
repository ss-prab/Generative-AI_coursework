# -*- coding: utf-8 -*-
"""Sneha Prabakar_Week_4_submission.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14hBKoWufA3hPs6EZbyPDBqmDfmlyIDTB

**Candidate Name:** Sneha Santha Prabakar

**Assignment:** Week 4 - CNN vs. ViT
"""

from google.colab import drive

drive.mount('/content/drive')

"""# Section 1: Import libraries and dataset"""

!pip install tensorflow
!pip install keras

# Core libraries
import random
import numpy as np
import pandas as pd

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# TensorFlow and Keras
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, regularizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Conv1D, GlobalMaxPooling1D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Set seeds for reproducibility
tf.random.set_seed(42)
random.seed(42)
np.random.seed(42)

"""## Import dataset

We start by importing the dataset, splitting it into test and train sets and normalizing it.

In order to **normalize** the data, we **divide it by 255** as this is an image data with pixel values typically in the range [0, 255] representing 8-bit color channels. Neural networks train more efficiently when inputs are on a standardized scale—ideally in the [0, 1] range or zero-centered like [-1, 1].

Here, we normalize the pixel values to be in the [0,1] range.
"""

# Load dataset
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()

print(f"\nx_train shape: {x_train.shape} and y_train shape: {y_train.shape}")
print(f"x_test shape: {x_test.shape} and y_test shape: {y_test.shape}")

# Normalize pixel values
x_train, x_test = x_train.astype("float32") / 255.0, x_test.astype("float32") / 255.0

# Class names
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

"""Total data size: 60,000 rows

Pixels: 32 x 32
  - Each image has a height and width of 32 pixels
  - This suggests that these are small, low-resolution images

Colour channels: 3
  - Each image has 3 colour channels - RGB (red, green, blue)

The training set contains 50,000 images
The test set contains 10,000 images

# Section 2: EDA visualization

## Class Distribution
"""

plt.figure(figsize=(10,5))
plt.hist(y_train, bins=np.arange(11) - 0.5, rwidth=0.8)
plt.xticks(np.arange(10), class_names, rotation=45)
plt.title("Class Distribution")
plt.xlabel("Class")
plt.ylabel("Count")
plt.show()

"""## Sample Images"""

plt.figure(figsize=(10, 5))
for i in range(10):
    plt.subplot(2, 5, i + 1)
    plt.imshow(x_train[i])
    plt.title(class_names[y_train[i][0]])
    plt.axis('off')
plt.suptitle("Sample Images from CIFAR-10")
plt.tight_layout()
plt.show()

"""The data has **10 classes** - ranging from different animals to automobiles.

We can see that the **class distribution is uniform** - all classes are equally represented in the dataset.

From visual analysis of the sample images we can make the following observations:

- **Diversity**: The CIFAR-10 dataset includes a variety of object types, such as animals (e.g., frog, deer, bird), vehicles (truck, automobile, ship), and scenes that differ significantly in texture and background complexity. Most of the images have complex, real-world backgrounds, which require the model to effectively separate foreground objects from context noise.

- **Low resolution**: Images are small (32x32 pixels), leading to visible blurring or pixelation. This may make it more difficult for the model to capture fine-grained details, such as distinguishing between a truck and an automobile.

- **Centered objects**: In most images, the objects are in the center of the frame, which can make classification easier for CNNs.

# Section 3: Analyse Data Quality

## Check for missing values
"""

print("Training data shape:", x_train.shape)
print("Missing values in x_train:", np.isnan(x_train).sum())
print("Missing values in y_train:", np.isnan(y_train).sum())

"""We can see that there are no missing values in the training data.

## Check for outliers
"""

print("Min pixel value:", x_train.min())
print("Max pixel value:", x_train.max())

"""We can see that there are no outliers in the training dataset as the minimum and maximum pixel values are 0 and 1 respectively (after normalization).

## Check for label range
"""

print("Unique labels in y_train:", np.unique(y_train))

"""There are 10 labels as expected - one for each in the following categories:
1. airplane
2. automobile
3. bird
4. cat
5. deer
6. dog
7. frog
8. horse
9. ship
10. truck

Overall, we can conclude that the data quality is generally good - there are no missing values or outliers or out-of-range labels.

# Step 4: Construct a CNN model

* We start with 3 convolutional layers to construct a moderately deep CNN as our data size is relatively small.

* We will also introduce Pooling after each convolutional block to reduce overfitting, improve efficiency, and distill key features.

* We will have just one hidden dense layer as it is sufficient when combined with convolutional layers. It balances model complexity vs. training time


Step-by-step explanation:

- Layer: Conv2d(32 filters)
  - Applies 32 filters of size 3x3 over the image to extract low-level features like edges, textures, and colors.
  - `Padding = same` spatial dimensions (32×32), which is useful for stacking layers without shrinking the image too quickly.
  - Activation function: `ReLU` (to introduce non-linearity)

- Layer: MaxPooling2D
  - Reduces the spatial size (height and width) by taking the maximum value in each 2x2 patch.
  - This Introduces spatial invariance, making the model more robust to shifts and distortions and it also prevents overfitting by reducing the feature map size.

- Layer: Conv2D(64 filters) and MaxPooling2D
  - Deeper layers learn more complex features (like shapes, object parts).
  - Increasing filters from 32 → 64 gives the network greater capacity to learn richer representations.
  - Two pooling layers help aggressively reduce spatial dimensions (from 32x32 to 8x8), which is important for final classification while retaining useful features.

- Layer: Flatten
  - Flattens the 3D feature maps (from previous layers) into a 1D vector so it can be fed into dense (fully connected) layers.

- Layer: Dense(64)
  - A fully connected layer with 64 neurons that combines all the extracted features to make sense of the patterns.
  - 64 units is a balanced choice as it is small enough to avoid overfitting, large enough to allow meaningful combinations.

- Layer: Dropout
  - Randomly drops 30% of the neurons during training.
  - Helps prevent overfitting.

- Layer: Dense(10)
  - Output layer with 10 neurons — one for each class in CIFAR-10.
  - Activation function: `Softmax` (to convert logits to probabilities, for the multiclass classification)
"""

cnn_model = models.Sequential([
    layers.Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3)),
    layers.Activation('relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64, (3, 3), padding='same'),
    layers.Activation('relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64, (3, 3), padding='same'),
    layers.Activation('relu'),

    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(10, activation='softmax')  # Output layer
])

"""# Step 5: Train the CNN and display its progress"""

cnn_model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

history_cnn = cnn_model.fit(x_train, y_train, epochs=30,
                            validation_data=(x_test, y_test))

cnn_model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

# Set Up Data Augmentation
datagen = ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True
)
datagen.fit(x_train)

early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

history_cnn = cnn_model.fit(datagen.flow(x_train, y_train, batch_size=64),
                            epochs=20,
                            validation_data=(x_test, y_test),
                            callbacks=[early_stop])

plt.plot(history_cnn.history['accuracy'], label='Train Acc')
plt.plot(history_cnn.history['val_accuracy'], label='Val Acc')
plt.legend()
plt.title("CNN Accuracy Over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.show()

"""We can make the following observation from the Accuracy curve:

- Training accuracy is steadily improves
"""

plt.plot(history_cnn.history["loss"], label="Train Loss")
plt.plot(history_cnn.history["val_loss"], label="Val Loss")
plt.title("CNN Loss Over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()

"""# Step 6: Construct a VIT model

We will construct a Vision Transformer (ViT) model based on the Keras official implementation and adapt it for the CIFAR-10 dataset:

Patch Extraction: Images of size 32×32 are divided into smaller patches of size 4×4, resulting in 64 patches per image.

Patch Encoding: Each patch is linearly projected into a feature vector and enriched with positional embeddings to retain spatial information.

Transformer Encoder: The encoded patches are passed through a stack of 4 transformer blocks, each consisting of:

  - Multi-Head Self Attention (2 heads)

  - Layer Normalization

  - Feedforward Neural Network (MLP)

  - Residual Connections

Key training decisions:

  - Optimizer: AdamW (Adam with weight decay)

  - Loss Function: Categorical crossentropy (with one-hot encoded labels)

  - Learning Rate Schedule: ReduceLROnPlateau

  - Regularization:

    - Dropout (0.2 in transformer and classifier layers)

    - EarlyStopping (to prevent unnecessary training after convergence)

    - Weight Decay (L2 regularization via AdamW)

## Configure hyperparameters
"""

# One-hot encode labels
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

num_classes = 10
input_shape = (32, 32, 3)

learning_rate = 5e-4
weight_decay = 1e-4
batch_size = 128
num_epochs = 25

image_size = 32
patch_size = 4
num_patches = (image_size // patch_size) ** 2
projection_dim = 32
num_heads = 2
transformer_units = [64, 32]       # Transformer MLP
transformer_layers = 4
mlp_head_units = [64, 32]          # Final classifier head
dropout_rate = 0.2

"""## Use Data Augmentation"""

data_augmentation = keras.Sequential([
    layers.Rescaling(1.0, input_shape=input_shape),
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.02),
    layers.RandomZoom(0.1)
])

"""## Implement Multiple Layer Perceptron (MLP)"""

def mlp(x, hidden_units, dropout_rate):
    for units in hidden_units:
        x = layers.Dense(units, activation="gelu")(x)
        x = layers.Dropout(dropout_rate)(x)
    return x

"""## Patching and encoding layers"""

class Patches(layers.Layer):
    def __init__(self, patch_size):
        super().__init__()
        self.patch_size = patch_size

    def call(self, images):
        batch_size = tf.shape(images)[0]
        patches = tf.image.extract_patches(
            images=images,
            sizes=[1, self.patch_size, self.patch_size, 1],
            strides=[1, self.patch_size, self.patch_size, 1],
            rates=[1, 1, 1, 1],
            padding="VALID",
        )
        patch_dims = patches.shape[-1]
        return tf.reshape(patches, [batch_size, -1, patch_dims])

class PatchEncoder(layers.Layer):
    def __init__(self, num_patches, projection_dim):
        super().__init__()
        self.projection = layers.Dense(units=projection_dim)
        self.position_embedding = layers.Embedding(input_dim=num_patches, output_dim=projection_dim)

    def call(self, patches):
        positions = tf.range(start=0, limit=tf.shape(patches)[1], delta=1)
        return self.projection(patches) + self.position_embedding(positions)

"""## Build the ViT Classifier"""

def create_vit_classifier():
    inputs = keras.Input(shape=input_shape)
    augmented = data_augmentation(inputs)
    patches = Patches(patch_size)(augmented)
    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)

    # Transformer blocks
    for _ in range(transformer_layers):
        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
        attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim)(x1, x1)
        x2 = layers.Add()([attention_output, encoded_patches])
        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)
        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=dropout_rate)
        encoded_patches = layers.Add()([x3, x2])

    # Classification head
    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
    representation = layers.GlobalAveragePooling1D()(representation)
    representation = layers.Dropout(dropout_rate)(representation)
    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=dropout_rate)
    logits = layers.Dense(num_classes, activation="softmax")(features)

    model = keras.Model(inputs=inputs, outputs=logits)
    return model

"""## Train the model"""

vit_model = create_vit_classifier()

optimizer = keras.optimizers.AdamW(
    learning_rate=learning_rate,
    weight_decay=weight_decay
)

vit_model.compile(
    optimizer=optimizer,
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

callbacks = [
    keras.callbacks.EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True),
    keras.callbacks.ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=3)
]

history = vit_model.fit(
    x_train, y_train,
    batch_size=batch_size,
    epochs=num_epochs,
    validation_split=0.1,
    callbacks=callbacks
)

"""## Run the model and plot"""

# Accuracy
plt.plot(history.history["accuracy"], label="Train Acc")
plt.plot(history.history["val_accuracy"], label="Val Acc")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("ViT Accuracy Over Epochs")
plt.legend()
plt.show()

# Loss
plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Val Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("ViT Loss Over Epochs")
plt.legend()
plt.show()

"""- Accuracy Curve:

Steady improvement in both training and validation accuracy.

Final training accuracy: ~55%

Final validation accuracy: ~55.5%

The closeness of the two curves suggests the model is not overfitting.

- Loss Curve:

Both training and validation loss decrease smoothly and converge around 1.27–1.29.

No divergence or sharp increase in validation loss, indicating stable generalization.

# Step 7: Comparison of the CNN and ViT Models

**Performace Gap**

- Final Train Accuracy: CNN (72.3%) >> ViT (55.2%)
- Final Validation Accuracy: CNN (75.8%) >> ViT (55.5%)
- Final Validation Loss: CNN (0.73) << ViT (1.28)

The CNN model outperformed the ViT by over 20 percentage points in validation accuracy.

CNNs are better suited for small datasets like CIFAR-10 due to their inductive bias (e.g., local spatial filtering), which allows them to generalize faster with fewer samples.

**Training Efficiency**

- Avg. Time per Epoch: CNN (6-8 seconds) << ViT (20-22 seconds)
- Convergence Rate: Much faster for CNN than ViT. CNN achieved good accuracy in less than 10 epochs, whereas ViT accuracy plateaued near epoch 20.

CNNs are more computationally efficient and reach higher accuracy faster.

ViTs took longer per epoch and more epochs to plateau — typical for transformer-based models without pretraining.

**Learning Pattern**

- Training vs. Validation accuracy:
  - CNN: Validation slightly higher - no overfit
  - ViT: Very close - well regularized
- Loss curve:
  - CNN: Smooth, sharp drop, stable
  - ViT: Smooth but plateaued early
- Generalisation ability:
  - CNN: Strong
  - ViT: Limited on this dataset due to no inductive bias or pretraining

CNN learned local features more quickly, helping it to generalise well.
ViT lacked inductive biases, requiring more data to perform competitively. Its accuracy plateaued early, suggesting it learned some meaningful representations, but not enough to compete with CNN.

**Final conclusion**

While the ViT model was correctly implemented and trained with attention mechanisms, regularization, and learning rate scheduling, it was outperformed by the CNN in nearly all aspects due to the nature of the dataset. This outcome aligns with known behavior of ViTs: they perform best with large datasets or pretrained weights, while CNNs are highly effective even on small datasets like CIFAR-10.
"""
# -*- coding: utf-8 -*-
"""Project_Sneha Santha Prabakar-1 (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XEPkQrnnhwAoXrcn9z3JKya6Arb72uzg

**Candidate name:** Sneha Santha Prabakar

**Capstone Project**

This notebook includes:
- Conversational memory chat
- PDF-based document QA using RAG
- Prompt-based image generation
- SQL query assistant
- Book recommendation agent
- Weather response agent
- Smart controller routing based on user input

It is **self-contained**, Colab-compatible, and meant to simulate a mini AI assistant.

**Executive Summary**

This project implements a multi-agent AI assistant system that demonstrates the integration of seven different AI capabilities through a unified controller. The system includes conversational memory, document querying with RAG, image generation, weather services, recommendation engines, and intelligent routing. Each component operates independently while being coordinated through a smart controller that analyzes user intent and routes requests to the appropriate agent, at the end.

The implementation showcases practical applications of current AI technologies including OpenAI‚Äôs GPT-4o, DALL-E 3, vector databases, and external API integrations, all working together to create a comprehensive AI assistant.

# Section 1: Environment Set-up
"""

# üì¶ Install all required packages
!pip install -U langchain langchain-community openai faiss-cpu sentence-transformers pypdf

# Import libraries

import os
from getpass import getpass

# Memory Chat
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
# from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI

# Document-querying and RAG Chat
from google.colab import files
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS

# Weather agent + SQL db + activities recommendation system
import requests
import openai
from datetime import datetime
import json
import sqlite3
import re

# üîê Secure API Key Handling
# We use `getpass()` so your OpenAI API key is never printed or stored in plaintext.

OPENAI_API_KEY = getpass("Enter your OpenAI API key: ")
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

WEATHER_API_KEY = getpass("Enter your Weather API key: ")
os.environ["WEATHER_API_KEY"] = WEATHER_API_KEY

TMDB_API_KEY = getpass("Enter your TMDB API key: ")
os.environ["TMDB_API_KEY"] = TMDB_API_KEY

GOOGLE_BOOKS_API_KEY = getpass("Enter your Google Books API key: ")
os.environ["GOOGLE_BOOKS_API_KEY"] = GOOGLE_BOOKS_API_KEY

"""# Section 2: Conversational Interface

**Goal:**

The goal of this section is to build a simple yet effective conversational AI assistant that can handle a natural, multi-turn dialogue with the user. It uses OpenAI‚Äôs GPT-4o, the most up-to-date large language model I have access to, to generate relevant and coherent responses.


**How it works:**

This setup uses LangChain‚Äôs `ConversationChain` combined with `ConversationBufferMemory`.

The `ConversationBufferMemory` keeps track of what the user says in each turn so that the assistant can remember the flow of the conversation.

The `ConversationChain` passes each user query, along with the stored memory, to the LLM - so follow-up questions can build on previous answers.

I specifically chose GPT-4o because it is the latest version with a more recent knowledge cutoff (2023) than older GPT-3.5 or GPT-4 models, which means it can provide more relevant, up-to-date general context when talking about broad topics like global AI governance.

**Result:**

The result is an interactive AI assistant bot that you can chat with by typing your questions. It can handle coherent turns and the conversation runs until you type exit to end it.

The `üë§ User` shows the user input.

And the `ü§ñ Assistant` shows the bot's response.

**Application:**

In the context of AI policy and governance, this kind of conversational interface can help:

Students, researchers, or policymakers explore broad concepts around AI governance and global strategies in a conversational way.

Test ideas and follow-up questions without needing to search multiple sources manually.

Serve as a prototype for how a real policy chatbot could assist with simple clarifications, explanations, or brainstorming sessions.

While this version only uses the general LLM, the same conversational memory approach can later be combined with document retrieval (RAG) or live tools to make the assistant more factual and context-specific. We will explore this in the next section of this notebook.
"""

# Create the conversation memory
simple_memory = ConversationBufferMemory()

# Use the plain OpenAI LLM
# llm_simple = OpenAI(temperature=0.5)

llm = ChatOpenAI(
    model_name="gpt-4o", # more up-to-date than a simple LLM model
    temperature=0
)

# Build the basic conversation chain
conversation = ConversationChain(
    llm=llm,
    memory=simple_memory
)


def memory_chat(user_prompt=""):
    if len(user_prompt):
        response = conversation.run(user_prompt)
        return f"ü§ñ Assistant: {response}\n"
    else:
        print("üí¨ AI Assistant - Ask Anything!")
        print("Type 'exit' to end the conversation.\n")
        while True:
            user_input = input("üë§ User: ")
            if user_input.lower() in ["exit", "quit"]:
                print("\n‚úÖ Conversation ended.")
                break
            response = conversation.run(user_input)
            print(f"ü§ñ Assistant: {response}\n")

memory_chat("What is Antartica?")

memory_chat("Where is it?")

memory_chat("What is it's population size?")

memory_chat("Hi again, what did I ask earlier?")

memory_chat()

"""# Section 3: Document-querying

**Goal:**

The goal of this section is to demonstrate how combining Retrieval-Augmented Generation (RAG) with conversational memory can make a language model more reliable when answering questions that require accurate, domain-specific information ‚Äî in this case, AI governance.

Instead of relying solely on what the model ‚Äúknows‚Äù from training, this approach ensures that answers are directly supported by a relevant policy document.

**Chosen Document: Singapore‚Äôs National AI Strategy 2.0**

https://file.go.gov.sg/nais2023.pdf

With Generative AI advancing so quickly, strong governance frameworks are more important than ever to manage risks like ethics, privacy, fairness, and accountability. Singapore‚Äôs National AI Strategy 2.0 (AI for Public Good) is a practical example of how a country is putting clear principles and safeguards in place to make sure AI benefits everyone while minimising potential harm. That makes it a meaningful document to test how an AI assistant can help people navigate complex policy information.

I chose this document because it ties directly to my research on Generative AI risk assessment, supply chain dynamics, and regulatory compliance. I focused on the introduction, the strategic pillars of the strategy, and especially System 3: Infrastructure & Environment, which sets out how Singapore is building the trusted environment and infrastructure needed for safe, responsible AI development and use.

By focusing on these sections, I can make sure the assistant retrieves meaningful, policy-relevant information ‚Äî not just generic or unrelated text.

**How it works:**

- The selected pages are loaded and divided into smaller, meaningful text chunks.

- Each chunk is embedded and stored in a vector database, enabling semantic search.

- For each user question, the system searches for the most relevant chunks.

- A Conversational Retrieval Chain combines these retrieved sections with the conversation history, allowing the assistant to keep track of previous questions and provide consistent answers.

- The language model then generates a final response that uses the retrieved content and the chat history to generate clear, factual answers.

**Result:**

There are two types of outcomes:

1. The assistant can handle direct, single-turn semantic queries, providing clear, factual answers based on the policy document. For example, I tested questions such as:

  - ‚ÄúWhat shifts is Singapore making from its first AI strategy to NAIS 2.0?‚Äù

  - ‚ÄúWhat is AI Verify and how does it support AI governance?‚Äù

  - ‚ÄúWhat is Singapore‚Äôs role in international AI governance efforts?‚Äù

These in-line queries show how the system retrieves the most relevant sections of the document and generates precise, contextual responses.

2. It also works as an interactive conversational assistant, where users can input follow-up questions in a multi-turn dialogue. The conversational memory ensures that the assistant keeps track of what has already been discussed, providing continuity and context across multiple queries - similar to how a real policy advisor might respond.

Together, these capabilities demonstrate how retrieval and memory can work hand in hand to make an AI assistant more accurate, context-aware, and useful for engaging with detailed governance frameworks.

**Application:**

This approach directly supports the Trusted Environment pillar of Singapore‚Äôs AI strategy by showing how complex policy information can be made more accessible and useful to the people who need it.

In practice, a tool like this could be used by:

- Policy teams and civil servants, who may need quick answers to detailed questions about AI risk levels, governance frameworks, or compliance measures.

Businesses and citizens, who want to better understand how national AI policies affect their work or daily lives.

Companies developing AI systems, who can use it to check that their solutions align with national guidelines on privacy, fairness, and accountability.

By combining retrieval with conversational memory, this assistant helps translate policy documents into practical, understandable information ‚Äî supporting Singapore‚Äôs goal of promoting AI for the public good.

## 2.1. User Upload - Policy Document

Source document: https://file.go.gov.sg/nais2023.pdf
"""

uploaded = files.upload()

"""## 2.2. Load the PDF, Filter Key Pages, and Chunk for Embeddings"""

# 1. Load the entire PDF
loader = PyPDFLoader("/content/SG AI for public good.pdf")
all_pages = loader.load()

print(f"Total pages in PDF: {len(all_pages)}")

# 2. Select only the key pages you want
# Note: PDF page 7 = index 6 (as Python is 0-indexed)
selected_pages = [
    all_pages[i] for i in list(range(6, 12))  # pages 7‚Äì12
    + list(range(14, 16))                     # pages 15‚Äì16
    + list(range(57, 64))                     # pages 58‚Äì64
]

print(f"Selected pages: {len(selected_pages)}")

# 3. Chunk the selected pages
splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)

chunks = splitter.split_documents(selected_pages)

print(f"Created {len(chunks)} chunks")

# 4. Display 3 sample chunks
for i, chunk in enumerate(chunks[:3]):
    print(f"\n--- Chunk {i+1} ---\n{chunk.page_content}\n")

"""This setup combines semantic retrieval with a LLM to build a practical Retrieval-Augmented Generation (RAG) system with conversational memory.

For every user query:
1. The retriever searches the vector store (FAISS) for the most relevant chunks from the embedded PDF.
2. It feeds these chunks, along with the user‚Äôs current question and the entire chat history, into the LLM‚Äôs prompt.
3. The LLM then generates a final answer that stays factual and context-aware, grounding its response in the retrieved policy content instead of relying only on its pre-trained knowledge.

This approach ensures the chatbot can handle multi-turn conversations while providing accurate, document-based answers. It‚Äôs especially useful when we want our assistant to stay aligned with a specific source ‚Äî for example, this Singapore National AI Strategy PDF ‚Äî rather than drifting off-topic.

## 2.3. Integrate Embeddings, Vector Store, and LLM for RAG Chat
"""

# Create embeddings and build the vector store:
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(chunks, embeddings)

# Initialize the LLM (using OpenAI Chat model) with zero temperature for factual answers
llm_context = ChatOpenAI(temperature=0)

# Create conversational memory to track chat history across turns
memory_context = ConversationBufferMemory(
    memory_key="chat_history", # Key used by LangChain to store conversation context
    return_messages=True) # Return previous messages for continuity

# Build the Conversational Retrieval Chain
# This combines the LLM, retriever (vector store), and memory into one chain
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm_context,
    retriever=vectorstore.as_retriever(),
    memory=memory_context
)

def document_q_n_a(question):
  return qa_chain.run(question)

# Test the chain with an example semantic query
question = "What are the three systems of Singapore's National AI Strategy 2.0?"
# answer = qa_chain.run(question)
answer = document_q_n_a(question)

# Display the user query and the assistant's generated answer
print(f"\nUser: {question}\nAssistant: {answer}")

"""## 2.4. Direct, single-turn semantic queries based on the document"""

questions = [
    "What shifts is Singapore making from its first AI strategy to NAIS 2.0",
    "What is AI Verify and how does it support AI governance?",
    "What is Singapore‚Äôs role in international AI governance efforts?"
]

for curr_q in questions:
    answer = document_q_n_a(curr_q)
    print(f"\nUser: {curr_q}\nAssistant: {answer}\n")

"""## 2.5. Context-Aware Interactive conversational assistant

Sample questions used to interact with the AI assistant bot:

What is AI?

What does the document say about this?

Can you summarise the actions?

How does this connect to privacy?
"""

print("üí¨ Start chatting with the AI assistant! (Type 'exit' to stop)\n")

while True:
    # Take user input each time
    user_question = input("üë§ User: ")

    # Check if the user wants to stop
    if user_question.lower() in ["exit", "quit"]:
        print("\n‚úÖ Chat ended.")
        break

    # Run the question through your Conversational Retrieval Chain
    answer = qa_chain.run(user_question)

    # Show the assistant's response
    print(f"ü§ñ Assistant: {answer}\n")

"""# Section 4: Image Generation with Prompt Engineering

**Goal:** Handle diverse prompts and conduct structured prompt experimentation when generating images with DALL¬∑E API

**How it works:** I have run a sequence of prompts that gradually adds scene complexity, technical specifications, and style changes ‚Äî so I could observe exactly how different wording and details affect the final result.

**Prompts used:**

1. `A person reading a book in a caf√©.`

I started with a very basic, generic prompt to see what the model produces when given minimal context.

This sets a clear baseline for comparison: What does the model default to in terms of the human subject, scene, and style when no extra detail is provided?

2. `A young woman reading a book by the window in a cozy caf√©, warm light, wooden furniture, cup of coffee on the table.`

Here, I added simple but important scene elements ‚Äî a window, warm lighting, furniture, and props ‚Äî to test how adding basic descriptive details improves atmosphere, realism, and narrative.

3. `A young woman with short hair reading a book by the window in a cozy caf√© during golden hour, warm sunlight streaming in, vintage d√©cor, cup of cappuccino, soft bokeh background, photo-realistic.`


This version pushes further by specifying time of day, lighting quality, d√©cor style, and mood, plus a photographic style (photo-realistic). I wanted to see how the AI interprets natural light and depth-of-field cues like bokeh.

4. `Portrait of a young woman reading a book by a caf√© window, shot on Canon EOS R5 with 50mm lens at f/1.4, shallow depth of field, golden hour natural light, cinematic mood, soft shadows, high detail.`

This prompt demonstrates technical prompt engineering using photography terms: specific camera model, lens, aperture, lighting, and mood. This shows how precise technical details can direct composition, focus, and realism ‚Äî testing the model‚Äôs ability to simulate real-world camera settings.

5. `Illustration of a young woman reading a book by a window in a cozy caf√©, warm color palette, soft painterly brushstrokes, storybook art style, calm atmosphere.`

Next, I switched the medium entirely to an illustration to test style transfer. Using words like storybook, painterly brushstrokes, and warm color palette shifts the output away from photorealistic and shows how descriptive style words impact artistic feel.

6. `A young woman reading a book in a cozy caf√© by the window, warm sunlight, cup of coffee ‚Äî no other people, no pets, no clutter, peaceful clean setting.`

This prompt tests negative prompting. By asking for no other people, no pets, no clutter, I explored whether the model can remove distractions and keep the scene focused and minimal.

7. `A young woman reading a book by a caf√© window, warm sunlight, cup of coffee, cozy atmosphere, oil painting on canvas, impressionist style, visible brushstrokes, warm earthy colors.`


This tests another medium: oil painting in an impressionist style. Here, I wanted to see how words like visible brushstrokes and warm earthy colors create a textured, traditional look compared to digital results.

8. `Overhead shot of a young woman reading a book in a cozy caf√©, black-and-white film style, high contrast, dramatic shadows, vintage mood, shot on Leica M6 with 35mm lens, natural light.`


This prompt combines an unusual composition (overhead shot) with black-and-white film simulation, vintage camera specs, and dramatic mood. It is to experiment with angle, lighting, and film style.

9. `A young woman reading a book in a cozy caf√© by the window, warm golden hour sunlight, cup of coffee, vintage d√©cor ‚Äî no clutter, no background people, no pets, clean minimal scene, peaceful quiet mood.`

Finally, I refined the previous negative prompt to show a clean minimal scene with stronger scene control. This tests how combining positive details (golden hour, vintage d√©cor) with negative constraints impacts the output clarity and composition.
"""

from openai import OpenAI

client = OpenAI()

# Function to generate image based on user_prompt
# Returns image url
def generate_image(prompt):

  response = client.images.generate(
        model="dall-e-3",
        prompt=prompt,
        size="1024x1024",
        quality="standard",
        n=1
    )

  return response.data[0].url

# Test prompts
prompts = [
    "A person reading a book in a caf√©.",
    "A young woman reading a book by the window in a cozy caf√©, warm light, wooden furniture, cup of coffee on the table.",
    "A young woman with short hair reading a book by the window in a cozy caf√© during golden hour, warm sunlight streaming in, vintage d√©cor, cup of cappuccino, soft bokeh background, photo-realistic.",
    "Portrait of a young woman reading a book by a caf√© window, shot on Canon EOS R5 with 50mm lens at f/1.4, shallow depth of field, golden hour natural light, cinematic mood, soft shadows, high detail.",
    "Illustration of a young woman reading a book by a window in a cozy caf√©, warm color palette, soft painterly brushstrokes, storybook art style, calm atmosphere.",
    "A young woman reading a book in a cozy caf√© by the window, warm sunlight, cup of coffee ‚Äî no other people, no pets, no clutter, peaceful clean setting.",
    "A young woman reading a book by a caf√© window, warm sunlight, cup of coffee, cozy atmosphere, oil painting on canvas, impressionist style, visible brushstrokes, warm earthy colors.",
    "Overhead shot of a young woman reading a book in a cozy caf√©, black-and-white film style, high contrast, dramatic shadows, vintage mood, shot on Leica M6 with 35mm lens, natural light.",
    "A young woman reading a book in a cozy caf√© by the window, warm golden hour sunlight, cup of coffee, vintage d√©cor ‚Äî no clutter, no background people, no pets, clean minimal scene, peaceful quiet mood."
]

for prompt in prompts:
    # response = client.images.generate(
    #     model="dall-e-3",
    #     prompt=prompt,
    #     size="1024x1024",
    #     quality="standard",
    #     n=1
    # )
    response = generate_image(prompt)
    print(f"Prompt: {prompt}")
    print(f"Image URL: {response}\n")

"""A detailed explanation for each image, together with the image itself, is included in the Capstone Report document.

However, from all the results combined, the **key general observations** are:

1. **Specific details shape realism**: Adding precise technical details ‚Äî such as camera specifications (‚ÄúCanon EOS R5, 50mm lens at f/1.4‚Äù) or lighting conditions (‚Äúgolden hour natural light‚Äù) ‚Äî produces more realistic depth of field, highlights, and shadows. This makes it much easier for the model to replicate the look and feel of real photography when clear, technical cues are included.
2. **Diffusion bias on human faces, even with artistic styles**: When art styles like ‚Äúimpressionist style,‚Äù ‚Äúoil painting,‚Äù or ‚Äúvisible brushstrokes‚Äù are used, clear textural changes are visible in the background and props ‚Äî but the human face often remains relatively photorealistic. This is likely because human faces are one of the most overrepresented patterns in training data, so models ‚Äúlearn‚Äù to render them smooth, symmetrical, and well-lit by default. To truly stylize the face, you must explicitly prompt for the artistic effect to apply to the subject‚Äôs facial features too.
3. **Negative prompts effectively control distractions**: Instructions such as ‚Äúno clutter,‚Äù ‚Äúno background people,‚Äù and ‚Äúno pets‚Äù are powerful for removing unwanted elements and keeping the scene focused. This shows how prompt engineering can constrain a diffusion model‚Äôs generative ‚Äúautopilot‚Äù and reduce irrelevant filler details.
4. **Angle and perspective influence more than just viewpoint**: For example, when the ‚Äúoverhead shot‚Äù angle was specified, the book naturally shifted from the table to the woman‚Äôs lap. This demonstrates that perspective instructions shape not only the camera viewpoint but also the subject‚Äôs pose and the spatial arrangement of props within the scene.
5. **Mood descriptors strengthen visual storytelling**: Descriptive phrases like ‚Äúcozy caf√©,‚Äù ‚Äúpeaceful quiet mood,‚Äù and ‚Äúvintage d√©cor‚Äù guide the model‚Äôs choices in colours, props, textures, and light placement. This results in more cohesive and believable compositions.
6. **Simple prompts invite default assumptions**: When prompts are vague ‚Äî for example, ‚Äúa person reading a book in a caf√©‚Äù ‚Äî the model tends to fill in common stereotypes, such as adding extra people, coffee cups, or generic cluttered backgrounds. More specific details help steer the output away from these generic defaults and reduce the influence of training data bias.
7. **Lack of diversity in subjects**: Throughout these tests, the generated images consistently portrayed the ‚Äúyoung woman‚Äù as fair-skinned, slim, and conventionally attractive, despite no explicit mention of race, body type, or appearance. This highlights the persistence of demographic bias and shows that prompts alone are often insufficient to produce diverse or representative outputs without intentional, explicit instructions.
8. **Stylization does not override default identity cues**: Switching to ‚Äústorybook illustration,‚Äù ‚Äúoil painting,‚Äù or ‚Äúvintage vibe‚Äù changes the style of the background and environment, but the woman‚Äôs appearance remains largely the same ‚Äî modern, casual clothing, minimal variation in facial features, and only slight changes in colour palette. This illustrates how strongly the model defaults to familiar, generic identity templates for humans.
9. **Order and structure of the prompt matters**: The sequence of words can affect which parts the model focuses on first. Putting subject first, then scene elements, then style or medium, tends to yield more consistent results. Example prompt structure:


`[Subject] + [Setting] + [Time of day] + [Lighting] + [Props] + [Mood] + [Style/Medium] + [Technical details] + [Negatives]`


**Importance for AI Policy and Regulation**


These observations underscore why transparent prompt design, systematic content auditing, and clear documentation are so essential when deploying generative AI tools, especially those capable of creating realistic human representations. Subtle patterns in the training data can easily reinforce stereotypes and cultural defaults without users even realising it. This has direct implications for fairness, accountability, and trust in AI systems.
From a governance perspective, it becomes clear that effective AI regulation should address not only how these models are trained and used, but also how the outputs are monitored for bias, representation, and unintended patterns. As more industries adopt generative AI, whether for creative work, communications, or policy support, the need for explainability, diversity-aware prompt engineering, and human oversight will be critical to ensure that AI-generated content reflects real-world values and serves the public good.

# Section 5: Weather Agent + Activities Recommendation Agent (using SQL)

## 5.1. Weather Agent
"""

# --------------------------
# 1) Function: get_weather
# --------------------------

def get_weather(location):
    """
    Calls the WeatherAPI to get current weather data for the given location.
    Handles API errors, invalid keys, invalid locations, timeouts, and unexpected responses.
    """
    try:
        # Build API request
        url = "http://api.weatherapi.com/v1/current.json"
        params = {
            "key": WEATHER_API_KEY,
            "q": location,
            "aqi": "no"
        }

        # Set a timeout to handle slow responses
        response = requests.get(url, params=params, timeout=5)
        response.raise_for_status()  # Raise exception for bad HTTP codes

        data = response.json()

        # Check if the response has expected keys
        if "location" not in data or "current" not in data:
            raise Exception("Unexpected response format from WeatherAPI.")

        return data

    except requests.exceptions.Timeout:
        raise Exception("WeatherAPI request timed out. Please try again later.")
    except requests.exceptions.HTTPError as e:
        if response.status_code == 401:
            raise Exception("Invalid WeatherAPI key. Please verify your key in config.py.")
        elif response.status_code == 400:
            raise Exception(f"Invalid location '{location}'. Please check your input.")
        else:
            raise Exception(f"WeatherAPI returned an HTTP error: {str(e)}")
    except requests.exceptions.RequestException as e:
        raise Exception(f"WeatherAPI connection error: {str(e)}")
    except Exception as e:
        raise Exception(f"WeatherAPI error: {str(e)}")

# --------------------------
# 2) OpenAI Function Definition
# --------------------------

# This tells OpenAI about the available function it can call.
functions = [
    {
        "name": "get_weather",
        "description": "Get current weather for a location.",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "City name or location (e.g., London, Singapore)"
                }
            },
            "required": ["location"]
        }
    }
]

# --------------------------
# 3) Main Weather Agent
# --------------------------
from openai import OpenAI, OpenAIError

def weather_agent(user_query):
    """
    Handles a user query by:
    1) Sending it to OpenAI.
    2) Letting the model decide if it wants to call get_weather.
    3) If so, it parses arguments, calls WeatherAPI, and returns formatted weather.
    Includes error handling for invalid inputs, missing keys, and network problems.
    """

    client = OpenAI()

    try:
        # Ask OpenAI if it wants to call the function
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": user_query}],
            functions=functions,
            function_call="auto"
        )

        # Extract the function call details
        function_call = response.choices[0].message.function_call

        if function_call and function_call.name == "get_weather":
            arguments = json.loads(function_call.arguments)

            # Ensure 'location' is provided
            location = arguments.get("location", "").strip()
            if not location:
                return "Error: No location provided."

            # Call WeatherAPI
            weather_data = get_weather(location)

            # Extract values with fallback
            city = weather_data.get('location', {}).get('name', 'Unknown location')
            temp_c = weather_data.get('current', {}).get('temp_c', '?')
            condition_text = weather_data.get('current', {}).get('condition', {}).get('text', 'Unknown')
            humidity = weather_data.get('current', {}).get('humidity', '?')
            wind_kph = weather_data.get('current', {}).get('wind_kph', '?')

            # Emoji map for weather condition
            condition = condition_text.lower()
            if ("sun" in condition) or ("clear" in condition):
                emoji = "‚òÄÔ∏è"
            elif ("cloud" in condition) or ("overcast" in condition):
                emoji = "‚òÅÔ∏è"
            elif "rain" in condition:
                emoji = "üåßÔ∏è"
            elif "snow" in condition:
                emoji = "‚ùÑÔ∏è"
            elif "thunder" in condition:
                emoji = "üå©Ô∏è"
            else:
                emoji = "üåà"

            # Return multi-line, user-friendly response
            return f"""\n{emoji} Current weather in {city}:
            Temperature: {temp_c}¬∞C
            Condition: {condition_text}
            Humidity: {humidity}%
            Wind: {wind_kph} kph"""

        else:
            return "No matching function was called by OpenAI."

    except openai.error.AuthenticationError:
        return "Error: Invalid OpenAI API key. Please check your config.py."
    except openai.error.OpenAIError as e:
        return f"OpenAI API error: {str(e)}"
    except Exception as e:
        return f"Error: {str(e)}"

"""## 5.2. Recommendation Agent

### 5.2.1 Recommendation for Museums and Events (Mock)

Mock data - for certain cities only
"""

def setup_museums_events_database():
    """
    Creates a 'city_activities.db' SQLite database with two tables:
    1) museums
    2) events
    Each table includes sample indoor/outdoor options for different cities.
    """

    conn = sqlite3.connect('city_activities.db')
    c = conn.cursor()

    # ---------- Museums Table ----------
    c.execute('''
        CREATE TABLE IF NOT EXISTS museums (
            id INTEGER PRIMARY KEY,
            name TEXT,
            type TEXT,      -- 'indoor'
            description TEXT,
            city TEXT,
            date TEXT       -- Mock field for consistency
        )
    ''')

    # Clear old rows
    c.execute('DELETE FROM museums')

    museums = [
        # Singapore
        ('National Gallery Singapore', 'indoor', 'Southeast Asian art collections', 'Singapore', '2025-07-16'),
        ('ArtScience Museum', 'indoor', 'Exhibitions exploring art and tech', 'Singapore', '2025-07-16'),

        # London
        ('British Museum', 'indoor', 'World history and artefacts', 'London', '2025-07-16'),
        ('Tate Modern', 'indoor', 'Modern and contemporary art', 'London', '2025-07-16'),

        # New York
        ('Metropolitan Museum of Art', 'indoor', 'Extensive art collections from around the world', 'New York', '2025-07-16'),
        ('Museum of Modern Art (MoMA)', 'indoor', 'Modern art and design exhibits', 'New York', '2025-07-16'),
    ]

    c.executemany('INSERT OR IGNORE INTO museums (name, type, description, city, date) VALUES (?,?,?,?,?)', museums)

    # ---------- Events Table ----------
    c.execute('''
        CREATE TABLE IF NOT EXISTS events (
            id INTEGER PRIMARY KEY,
            name TEXT,
            type TEXT,      -- 'indoor' or 'outdoor'
            description TEXT,
            city TEXT,
            date TEXT
        )
    ''')

    # Clear old rows
    c.execute('DELETE FROM events')

    events = [
        # Singapore
        ('Marina Bay Sands Light Show', 'outdoor', 'Evening light and water show at Marina Bay', 'Singapore', '2025-07-16'),
        ('Esplanade Concert', 'indoor', 'Classical music concert', 'Singapore', '2025-07-16'),

        # London
        ('Hyde Park Summer Festival', 'outdoor', 'Live music and food stalls', 'London', '2025-07-16'),
        ('West End Musical', 'indoor', 'Popular stage musical in the theatre district', 'London', '2025-07-16'),

        # New York
        ('Central Park Jazz Night', 'outdoor', 'Open-air jazz performance', 'New York', '2025-07-16'),
        ('Broadway Play', 'indoor', 'New Broadway theatre production', 'New York', '2025-07-16'),
    ]

    c.executemany('INSERT OR IGNORE INTO events (name, type, description, city, date) VALUES (?,?,?,?,?)', events)

    conn.commit()
    conn.close()
    print("‚úÖ city_activities.db created with sample museums and events.")

# Run this once to set up the DB
if __name__ == "__main__":
    setup_museums_events_database()

# -------------------------------
# LOCAL MUSEUMS & EVENTS AGENT
# -------------------------------

def get_local_activities(city, activity_type):
    conn = sqlite3.connect('city_activities.db')
    c = conn.cursor()

    museums = []
    events = []

    # Museums are always indoor
    c.execute("SELECT name, description FROM museums WHERE city = ?", (city,))
    museums = c.fetchall()

    # Events can be indoor or outdoor
    c.execute("SELECT name, description FROM events WHERE city = ? AND type = ?", (city, activity_type))
    events = c.fetchall()

    conn.close()

    return museums, events

def get_sql_tables_local_activities(prompt):
    conn = sqlite3.connect('city_activities.db')
    c = conn.cursor()

    museums = []
    events = []


    # Table: Museums
    if "museums" in prompt:
      c.execute("SELECT * FROM museums;")
      result = c.fetchall()

    # Table: Events
    else:
      c.execute("SELECT * FROM events;")
      result = c.fetchall()

    conn.close()

    return result

def extract_city(prompt):
    cities_list = ["Singapore", "London", "New York"]
    for city in cities_list:
        if city.lower() in prompt.lower():
            return city
    return None


def get_places_recommendation(prompt):

  # remove symbols like question mark, that may be attached to the city/activity type in the prompt question
  cleaned_prompt = re.sub(r'[^A-Za-z0-9\s]', '', prompt)

  city = extract_city(cleaned_prompt)
  activity_type = "outdoor" if "outdoor" in cleaned_prompt.lower() else "indoor"

  return get_local_activities(city, activity_type)

"""### 5.2.2. Recommendation for Books (using Google Books API)

The Google Books API allows us to filter books based on the user's preference of Genre. This function gives the top 3 recommendations of books that the user can choose from.

Example of genres that the user can filter the books by:

- Science fiction

- Fantasy

- Horror

- Mystery

- Romance

- Art

- Business

- History

- Self-Help

- Computers

- Design

- Food

- Geography

- Children

- Cooking

- Politics

- Comedy

- Biography
"""

def recommend_books_google(genre=None, language="en"):
    api_url = "https://www.googleapis.com/books/v1/volumes"

    # Use the genre directly in the search query
    if genre:
        query = f"subject:{genre}"
    else:
        query = "bestseller" # default genre - pick bestsellers, if the user has not given any input

    params = {
        "q": query,
        "langRestrict": language,
        "maxResults": 10
    }

    response = requests.get(api_url, params=params)
    response.raise_for_status()
    books = response.json().get("items", [])

    # Basic quality filters
    filtered_books = []
    for book in books:
        info = book.get("volumeInfo", {})
        page_count = info.get("pageCount", 0)
        description = info.get("description", "")
        avg_rating = info.get("averageRating", 0)

        if page_count < 30:
            continue
        if len(description) < 50:
            continue
        if avg_rating and avg_rating < 3:
            continue

        filtered_books.append(info)

    if not filtered_books:
        return [f"No suitable books found for genre '{genre}' in language '{language}'. Try a broader term or different genre."]

    # Output: simple list
    results = []
    for info in filtered_books[:5]:  # limit to top 5
        title = info.get("title", "No title")
        authors = ", ".join(info.get("authors", [])) if info.get("authors") else "Unknown Author"
        results.append(f"{title} by {authors}")

    return results

def detect_book_genre(prompt):
    known_genres = [
        "science fiction", "fantasy", "horror", "mystery", "romance",
        "art", "business", "history", "self-help", "computers",
        "design", "food", "geography", "children", "cooking", "politics"
    ]

    cleaned_prompt = re.sub(r'[^A-Za-z0-9\s]', '', prompt)
    prompt_lower = cleaned_prompt.lower()

    for genre in known_genres:
        if genre in prompt_lower:
            return genre  # Return the first match found

    return None

def get_books_recommendation(prompt):

  genre = detect_book_genre(prompt)
  return recommend_books_google(genre)

recommend_books_google(genre="Science")

recommend_books_google(genre="Biography")

"""### 5.2.3. Recommendation for Movies (using TMDB API)"""

# Genre ID map for user-friendly input
GENRE_ID_MAP = {
    "action": 28,
    "adventure": 12,
    "animation": 16,
    "comedy": 35,
    "crime": 80,
    "documentary": 99,
    "drama": 18,
    "family": 10751,
    "fantasy": 14,
    "history": 36,
    "horror": 27,
    "music": 10402,
    "mystery": 9648,
    "romance": 10749,
    "science fiction": 878,
    "sci-fi": 878,
    "tv movie": 10770,
    "thriller": 53,
    "war": 10752,
    "western": 37
}


def recommend_movies_tmdb(genre=None, language="en", max_results=5):
    """
    Query TMDb API with genre and language filters,
    plus quality filters for better results.
    """
    base_url = "https://api.themoviedb.org/3/discover/movie"

    genre_id = GENRE_ID_MAP.get(genre.lower()) if genre else None

    params = {
        "api_key": TMDB_API_KEY,
        "sort_by": "vote_average.desc",      # Sort by rating instead of raw popularity
        "vote_count.gte": 50,                # Only movies with 50+ votes
        "vote_average.gte": 5.5,             # Minimum average rating
        "include_adult": False,              # Skip adult content
        "language": language,                # Metadata language
        "with_original_language": language,  # Original movie language
        "page": 1
    }

    if genre_id:
        params["with_genres"] = genre_id

    try:
        response = requests.get(base_url, params=params, timeout=5)
        response.raise_for_status()
        data = response.json()

        if "results" not in data or not data["results"]:
            return ["No movies found with these filters."]

        results = []
        for movie in data["results"][:max_results]:
            title = movie.get("title", "Untitled")
            year = movie.get("release_date", "")[:4] if movie.get("release_date") else "N/A"
            # rating = movie.get("vote_average", "N/A")
            results.append(f"‚Äò{title}‚Äô ({year})")

        return results

    except requests.RequestException as e:
        return [f"Error fetching movies: {str(e)}"]

def detect_movie_genre(prompt):
    known_movie_genres = [
        "action", "adventure", "animation", "comedy", "crime",
        "documentary", "drama", "family", "fantasy", "history",
        "horror", "music", "mystery", "romance", "science fiction",
        "sci-fi", "tv movie", "thriller", "war", "western"
    ]

    cleaned_prompt = re.sub(r'[^A-Za-z0-9\s]', '', prompt)
    prompt_lower = cleaned_prompt.lower()
    for genre in known_movie_genres:
        if genre in prompt_lower:
            return genre  # Return the first genre found

    return None


def get_movie_recommendation(prompt):

  genre = detect_movie_genre(prompt)
  return recommend_movies_tmdb(genre)

recommend_movies_tmdb(genre="romance")

"""### 5.2.4. Weather-based recommendation"""

def simple_weather_report(user_prompt=""):

  weather_summary = weather_agent(user_prompt)
  print(weather_summary)

simple_weather_report("what is the weather in egypt?")

def weather_activities_agent(user_prompt=""):
    print("üå¶Ô∏è Welcome! Let‚Äôs start by checking the weather.")

    # 1) Get weather info
    if(len(user_prompt)<1):
      location = input("Enter your city: ")
      weather_summary = weather_agent(f"What‚Äôs the weather like in {location}?")
      print(weather_summary)

    else:
      cleaned_prompt = re.sub(r'[^A-Za-z0-9\s]', '', user_prompt)
      weather_summary = weather_agent(cleaned_prompt)
      print(weather_summary)
      cities_list = ["Singapore", "London", "New York"]
      for city in cities_list:
          if city.lower() in user_prompt.lower():
              location = city
              continue

    # 2) Ask if user wants recommendations
    user_choice = input("\nWould you like activity recommendations based on today‚Äôs weather? (yes/no): ").strip().lower()
    if user_choice != "yes":
        print("‚úÖ All done! Have a nice day!")
        return

    # 3) Decide activity type
    bad_weather_keywords = ["rain", "storm", "thunder", "cloudy"]

    activity_type = "indoor" if any(word in weather_summary.lower() for word in bad_weather_keywords) else "outdoor"

    print(f"\nüí° Based on the weather, I‚Äôll suggest {activity_type} activities.")

    # 4) Get museums & events
    museums, events = get_local_activities(location, activity_type)
    print("\nüèõÔ∏è Local Museums:")
    if museums:
        for m in museums:
            print(f"- {m[0]}: {m[1]}")
    else:
        print("No museums found found in this city.")

    print("\nüéâ Local Events:")
    if events:
        for e in events:
            print(f"- {e[0]}: {e[1]}")
    else:
        print("No events found in this city.")

    # 5) Books
    genre_books = input("\nüìö Do you have a preferred book genre? (leave blank for no filter): ")
    books = recommend_books_google(genre=genre_books if genre_books else None)
    print("\nüìö Book Recommendations:")
    for b in books:
        print(f"- {b}")

    # 6) Movies
    genre_movies = input("\nüé¨ Do you have a preferred movie genre? (leave blank for no filter): ")
    movies = recommend_movies_tmdb(genre=genre_movies if genre_movies else None)
    print("\nüé¨ Movie Recommendations:")
    for m in movies:
        print(f"- {m}")



    print("\n‚úÖ Done! Enjoy your activities!")

weather_activities_agent()

"""# Section 6: Smart Multi-Agent Controller"""

# Smart Controller
# Detects user intent and routes input to the right agent/module

def controller(user_input):
    prompt = user_input.lower()

    if "weather" in prompt:
      if("activities" in prompt):
        return weather_activities_agent(prompt)
      else:
        return simple_weather_report(prompt)
    elif ((("recommend" in prompt) and ("places" in prompt)) or (("suggest" in prompt) and ("places" in prompt))):
        return get_places_recommendation(prompt)
    elif ((("recommend" in prompt) and ("movies" in prompt)) or (("suggest" in prompt) and ("movies" in prompt))):
        return get_movie_recommendation(prompt)
    elif ((("recommend" in prompt) and ("books" in prompt)) or (("suggest" in prompt) and ("books" in prompt))):
        return get_books_recommendation(prompt)
    elif "image" in prompt or "generate" in prompt or "draw" in prompt:
        return generate_image(user_input)
    elif "sql" in prompt:
        return get_sql_tables_local_activities(prompt)
    elif "Singapore AI" in prompt or "document" in prompt or "pdf" in prompt or "Singapore AI plan" or "Singapore's" or "National AI" in prompt:
        return document_q_n_a(user_input)
    else:
      return memory_chat(user_input)

def section_break():
    print("\n" + "="*50)

# Test All Capabilities

print("üß† Memory Chat:")
print(controller("Hi, how many continents are there?"))
section_break()

print("üå§ Simple Weather Info:")
controller("What's the weather like in Singapore?")
section_break()

print("üå§üé≠ Interactive Weather Info:")
controller("Recommend activities based on today's weather in New York.")
section_break()

print("üìö Book Suggestions:")
print(controller("Can you recommend some History books?"))
section_break()

print("üçø Movie Suggestions:")
print(controller("Can you recommend some Comedy movies?"))
section_break()

print("üìä SQL Data:")
print(controller("Show me the Museums SQL query."))
section_break()

print("üèõÔ∏è Places to visit Suggestions:")
print(controller("Can you recommend some places to visit in London?"))
section_break()

print("üé® Generate an image of a smiling person")
print(controller("Generate an image of a smiling person"))
section_break()

print("üß† Memory Chat:")
print(controller("Hi again, what did I ask earlier?"))

print("üìÑ Document QA:")
print(controller("What is Singapore's National AI Strategy 2.0?"))
section_break()

"""# Conclusion

This multi-agent AI assistant system successfully demonstrates the integration of seven distinct AI capabilities through a unified controller architecture. The implementation showcases practical applications of current AI technologies while maintaining system modularity and user-friendly interaction patterns.


**Key Technical Contributions**
* Smart Controller: Intelligent intent recognition and routing
* Memory Integration: Consistent context across different agents
* Multi-Modal Capabilities: Text, image, and data processing
* External API Coordination: Robust integration with multiple services


**System Strengths**
* Comprehensive Functionality: Covers diverse AI use cases
* Modular Design: Easy to extend and modify
* Robust Architecture: Handles failures gracefully
* User-Friendly Interface: Natural language interaction


The system demonstrates how modern AI technologies can be effectively combined to create sophisticated, multi-functional AI assistants that maintain coherent user experiences across diverse capabilities.
"""